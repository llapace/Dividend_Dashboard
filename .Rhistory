threshold_leverage <- 2 * (length(coef(model2_step_log)) + 1) / n
threshold_residuals <- 2  # Commonly used standard threshold for residuals
# Identifying each type of point
outliers <- which(abs(std_residuals) > threshold_residuals)
high_leverage <- which(leverage_values > threshold_leverage)
influential <- which(cooks_values > threshold_cooks)
# Output the results
cat("Outliers (|Standardized Residuals| > 2):", outliers, "\n")
cat("High-Leverage Points (Leverage > 2*(k+1)/n):", high_leverage, "\n")
cat("Influential Points (Cook's Distance > 4/n):", influential, "\n")
#2c
influence_measures <- influence.measures(model2_step_log)
#standardized residuals
std_residuals <- rstandard(model2_step_log)
#leverage values
leverage_values <- hatvalues(model2_step_log)
#cooks Distance
cooks_values <- influence_measures$cooks.distance
#thresholds for identifying outliers and influential points
n <- nrow(data)
threshold_cooks <- 4 / n
threshold_leverage <- 2 * (length(coef(model2_step_log)) + 1) / n
threshold_residuals <- 2
# Identifying each type of point
outliers <- which(abs(std_residuals) > threshold_residuals)
high_leverage <- which(leverage_values > threshold_leverage)
influential <- which(cooks_values > threshold_cooks)
cat("Outliers (|Standardized Residuals| > 2):", outliers, "\n")
cat("High-Leverage Points (Leverage > 2*(k+1)/n):", high_leverage, "\n")
cat("Influential Points (Cook's Distance > 4/n):", influential, "\n")
#2d
# Assuming we identify the most influential based on Cook's distance even if it's below the typical threshold
most_influential <- which.max(cooks_values)  # Assuming cooks_values is still available in your environment
# Exclude the most influential point
data_without_most_influential <- data[-most_influential, ]
# Refit the model without this single most influential point
model2_refined_single <- lm(log(sales) ~ TV + radio + newspaper, data = data_without_most_influential)
#1
library(ISwR)
library(boot)
data(thuesen)
attach(thuesen)
model <- lm(short.velocity ~ blood.glucose)
#classical approach to obtain standard error and confidence intervals
classical_se <- summary(model)$coefficients["blood.glucose", "Std. Error"]
classical_ci <- confint(model, "blood.glucose", level = 0.95)
#regression coefficient of blood glucose
boot_fn <- function(data, index) {
fit <- lm(short.velocity ~ blood.glucose, data = data, subset = index)
return(coef(fit)[2])  # Returning the coefficient of blood glucose
}
#set up bootstrap
set.seed(123)  # for reproducibility
results <- boot(data = thuesen, statistic = boot_fn, R = 1000)
#bootstrap standard error and confidence interval
bootstrap_se <- sd(results$t)
bootstrap_ci <- boot.ci(results, type = "perc", index = 2)$percent[4:5]
#1
library(ISwR)
library(boot)
# Load the Thuesen dataset
data(thuesen)
attach(thuesen)
# Define the regression model
model <- lm(short.velocity ~ blood.glucose)
# Classical approach to obtain standard error and confidence intervals
classical_se <- summary(model)$coefficients["blood.glucose", "Std. Error"]
classical_ci <- confint(model, "blood.glucose", level = 0.95)
# Function to obtain the regression coefficient of blood glucose
boot_fn <- function(data, index) {
fit <- lm(short.velocity ~ blood.glucose, data = data, subset = index)
return(coef(fit)["blood.glucose"])  # Ensure we return the correct coefficient
}
# Set up bootstrap with 1000 replications
set.seed(123)  # for reproducibility
results <- boot(data = thuesen, statistic = boot_fn, R = 1000)
# Extracting bootstrap standard error and confidence interval
bootstrap_se <- sd(results$t)
bootstrap_ci <- boot.ci(results, type = "perc")$percent[4:5]  # Adjusted index referencing
# Print results
cat("Classical Standard Error:", classical_se, "\n")
cat("Classical 95% Confidence Interval:", classical_ci, "\n")
cat("Bootstrap Standard Error:", bootstrap_se, "\n")
cat("Bootstrap 95% Confidence Interval:", bootstrap_ci[,1], "to", bootstrap_ci[,2], "\n")
#1
# Load necessary libraries
library(ISwR)
library(boot)
# Detach any previous data to avoid masking
detach("package:thuesen", unload = TRUE)
#1
# Load necessary libraries
library(ISwR)
library(boot)
# Load the Thuesen dataset
data("thuesen")
# It's better practice not to use attach(); reference data directly
# Define the regression model using the data directly
model <- lm(short.velocity ~ blood.glucose, data = thuesen)
# Classical approach to obtain standard error and confidence intervals
classical_se <- summary(model)$coefficients["blood.glucose", "Std. Error"]
classical_ci <- confint(model, "blood.glucose", level = 0.95)
# Function to obtain the regression coefficient of blood glucose
boot_fn <- function(data, index) {
fit <- lm(short.velocity ~ blood.glucose, data = data, subset = index)
return(coef(fit)["blood.glucose"])  # Ensure we return the correct coefficient
}
# Set up bootstrap with 1000 replications
set.seed(123)  # for reproducibility
results <- boot(data = thuesen, statistic = boot_fn, R = 1000)
# Extracting bootstrap standard error
bootstrap_se <- sd(results$t)
# Extracting bootstrap confidence interval
bootstrap_ci <- boot.ci(results, type = "perc")
bootstrap_ci_range <- bootstrap_ci$percent[4:5]
# Print results
cat("Classical Standard Error:", classical_se, "\n")
cat("Classical 95% Confidence Interval:", classical_ci[1], "to", classical_ci[2], "\n")
cat("Bootstrap Standard Error:", bootstrap_se, "\n")
cat("Bootstrap 95% Confidence Interval:", bootstrap_ci_range[1], "to", bootstrap_ci_range[2], "\n")
#1
library(ISwR)
library(boot)
data("thuesen")
model <- lm(short.velocity ~ blood.glucose, data = thuesen)
#classical approach
classical_se <- summary(model)$coefficients["blood.glucose", "Std. Error"]
classical_ci <- confint(model, "blood.glucose", level = 0.95)
#regression coefficient of blood glucose
boot_fn <- function(data, index) {
fit <- lm(short.velocity ~ blood.glucose, data = data, subset = index)
return(coef(fit)["blood.glucose"])  # Ensure we return the correct coefficient
}
set.seed(123)  # for reproducibility
results <- boot(data = thuesen, statistic = boot_fn, R = 1000)
#bootstrap standard error
bootstrap_se <- sd(results$t)
#bootstrap confidence intervals
bootstrap_ci <- boot.ci(results, type = "perc")
bootstrap_ci_range <- bootstrap_ci$percent[4:5]
cat("Classical Standard Error:", classical_se, "\n")
cat("Classical 95% Confidence Interval:", classical_ci[1], "to", classical_ci[2], "\n")
cat("Bootstrap Standard Error:", bootstrap_se, "\n")
cat("Bootstrap 95% Confidence Interval:", bootstrap_ci_range[1], "to", bootstrap_ci_range[2], "\n")
print("Bootstrap approach estimates variability & distribution of ð›½^1 by resampling data, yielding robust SE & CI estimates. Particularly useful when data violate normality assumptions or are influenced by outliers. Bootstrap intervals tend to be wider for skewed or heteroscedastic data, more accurately reflecting parameter estimate uncertainty, possibly including zero where classical intervals don't.")
#2d
# Assuming 'influential' contains the indices of influential points and is not empty
most_influential_index <- influential[which.max(cooks_values[influential])]  # Most influential based on the highest Cook's distance
# Fit the model with the influential outlier
model_with <- model2_step_log  # As already created in 2b, which includes all observations
# Exclude the most influential outlier and fit the model
data_without_influential <- data[-most_influential_index, ]  # Exclude by index
model_without <- lm(log(sales) ~ TV + radio + newspaper, data = data_without_influential)
#2d
# Assuming 'influential' contains the indices of influential points and is not empty
most_influential_index <- influential[which.max(cooks_values[influential])]
if (length(most_influential_index) > 1) {
most_influential_index <- most_influential_index[1]  # Choosing the first if multiple are returned
}
# Fit the model with the influential outlier
model_with <- model2_step_log  # As already created in 2b, which includes all observations
# Exclude the most influential outlier and fit the model
data_without_influential <- data[-most_influential_index, ]  # Exclude by index
model_without <- lm(log(sales) ~ TV + radio + newspaper, data = data_without_influential)
#2d
# Assuming `std_residuals` and `leverage_values` are already calculated:
high_residuals <- which(abs(std_residuals) > 2)
high_leverage <- which(leverage_values > (2 * (length(coef(model2_step_log)) + 1) / n))
# Find the intersection of both high residuals and high leverage
most_influential_candidates = intersect(high_residuals, high_leverage)
# If there are multiple, you can choose one based on the highest absolute residual
if (length(most_influential_candidates) > 0) {
most_influential_index <- most_influential_candidates[which.max(abs(std_residuals[most_influential_candidates]))]
} else {
cat("No observation meets the criteria for being influential based on residuals and leverage.\n")
}
# Fit the model with the influential outlier
model_with <- model2_step_log  # As already created, which includes all observations
# Exclude the most influential outlier and fit the model
if (exists("most_influential_index")) {
data_without_influential <- data[-most_influential_index, ]  # Exclude by index
model_without <- lm(log(sales) ~ TV + radio + newspaper, data = data_without_influential)
# Summary of models for comparison
summary_with <- summary(model_with)
summary_without <- summary(model_without)
# Display the changes
cat("With the influential outlier:\n")
print(summary_with)
cat("Without the influential outlier:\n")
print(summary_without)
} else {
cat("No influential outlier identified to exclude.\n")
}
#2d
# Assuming 'influential' contains the indices of influential points and is not empty
most_influential_index <- which.max(abs(std_residuals))
# Fit the model with the influential outlier
model_with <- model2_step_log  # As already created in 2b, which includes all observations
# Exclude the most influential outlier and fit the model
data_without_influential <- data[-most_influential_index, ]  # Exclude by index
model_without <- lm(log(sales) ~ TV + radio + newspaper, data = data_without_influential)
# Summary of models for comparison
summary_with <- summary(model_with)
summary_without <- summary(model_without)
#2d
# Assuming 'std_residuals' are already calculated:
max_residual_index <- which.max(abs(std_residuals))  # Index of the maximum absolute standardized residual
# Fit the model with all data including the outlier
model_with_outlier <- model2_step_log  # Assuming model2_step_log includes all observations
# Exclude the outlier and fit the model
data_without_outlier <- data[-max_residual_index, ]  # Remove the outlier by index
model_without_outlier <- lm(log(sales) ~ TV + radio + newspaper, data = data_without_outlier)
# Summaries for comparison
summary_with_outlier <- summary(model_with_outlier)
summary_without_outlier <- summary(model_without_outlier)
#2d
max_residual_index <- which.max(abs(std_residuals))  # Index of the maximum absolute standardized residual
model_with_outlier <- model2_step_log  # Assuming model2_step_log includes all observations
data_without_outlier <- data[-max_residual_index, ]  # Remove the outlier by index
model_without_outlier <- lm(log(sales) ~ TV + radio + newspaper, data = data_without_outlier)
summary_with_outlier <- summary(model_with_outlier)
print(summary_with_outlier)
summary_without_outlier <- summary(model_without_outlier)
print(summary_without_outlier)
#1
library(ISwR)
library(boot)
data("thuesen")
model <- lm(short.velocity ~ blood.glucose, data = thuesen)
#classical approach
classical_se <- summary(model)$coefficients["blood.glucose", "Std. Error"]
classical_ci <- confint(model, "blood.glucose", level = 0.95)
#regression coefficient of blood glucose
boot_fn <- function(data, index) {
fit <- lm(short.velocity ~ blood.glucose, data = data, subset = index)
return(coef(fit)["blood.glucose"])  # Ensure we return the correct coefficient
}
set.seed(123)  # for reproducibility
results <- boot(data = thuesen, statistic = boot_fn, R = 1000)
#bootstrap standard error
bootstrap_se <- sd(results$t)
#bootstrap confidence intervals
bootstrap_ci <- boot.ci(results, type = "perc")
bootstrap_ci_range <- bootstrap_ci$percent[4:5]
cat("Classical Standard Error:", classical_se, "\n")
cat("Classical 95% Confidence Interval:", classical_ci[1], "to", classical_ci[2], "\n")
cat("Bootstrap Standard Error:", bootstrap_se, "\n")
cat("Bootstrap 95% Confidence Interval:", bootstrap_ci_range[1], "to", bootstrap_ci_range[2], "\n")
print("Bootstrap approach estimates variability & distribution of ð›½^1 by resampling data, yielding robust SE & CI estimates. Particularly useful when data violate normality assumptions or are influenced by outliers. Bootstrap intervals tend to be wider for skewed or heteroscedastic data, more accurately reflecting parameter estimate uncertainty, possibly including zero where classical intervals don't.")
#1
library(ISwR)
library(boot)
data("thuesen")
model <- lm(short.velocity ~ blood.glucose, data = thuesen)
#classical approach
classical_se <- summary(model)$coefficients["blood.glucose", "Std. Error"]
classical_ci <- confint(model, "blood.glucose", level = 0.95)
#regression coefficient of blood glucose
boot_fn <- function(data, index) {
fit <- lm(short.velocity ~ blood.glucose, data = data, subset = index)
return(coef(fit)["blood.glucose"])  # Ensure we return the correct coefficient
}
set.seed(123)  # for reproducibility
results <- boot(data = thuesen, statistic = boot_fn, R = 1000)
#bootstrap standard error
bootstrap_se <- sd(results$t)
#bootstrap confidence intervals
bootstrap_ci <- boot.ci(results, type = "perc")
bootstrap_ci_range <- bootstrap_ci$percent[4:5]
cat("Classical Standard Error:", classical_se, "\n")
cat("Classical 95% Confidence Interval:", classical_ci[1], "to", classical_ci[2], "\n")
cat("Bootstrap Standard Error:", bootstrap_se, "\n")
cat("Bootstrap 95% Confidence Interval:", bootstrap_ci_range[1], "to", bootstrap_ci_range[2], "\n")
print("Bootstrap approach estimates variability & distribution of ð›½^1 by resampling data, yielding robust SE & CI estimates. Particularly useful when data violate normality assumptions or are influenced by outliers. Bootstrap intervals tend to be wider for skewed or heteroscedastic data, more accurately reflecting parameter estimate uncertainty, possibly including zero where classical intervals don't.")
#2 Bonus
#Original coefficient from the model
original_coef <- coef(model)["blood.glucose"]
# Function to extract coefficient from bootstrap samples
boot_coef <- function(data, index) {
fit <- lm(short.velocity ~ blood.glucose, data = data[index, ])
return(coef(fit)["blood.glucose"])
}
# Calculate bootstrap coefficients
bootstrap_coef <- boot(data = thuesen, statistic = boot_coef, R = 1000)
# Calculate bootstrap p-value
bootstrap_p_value <- mean(abs(bootstrap_coef$t) >= abs(original_coef))
cat("Bootstrap p-value:", bootstrap_p_value, "\n")
#2 Bonus
original_coef <- coef(model)["blood.glucose"]
boot_coef <- function(data, index) {
fit <- lm(short.velocity ~ blood.glucose, data = data[index, ])
return(coef(fit)["blood.glucose"])
}
bootstrap_coef <- boot(data = thuesen, statistic = boot_coef, R = 1000)
bootstrap_p_value <- mean(abs(bootstrap_coef$t) >= abs(original_coef))
cat("Bootstrap p-value:", bootstrap_p_value, "\n")
print(" the bootstrap p-value of 0.474 suggests that the relationship between blood glucose and short velocity may not be statistically significant based on the available data and model.")
library(readr)
data <- read_csv("~/Spring_2024/Applied_Stats_2/HW/hw9/Advertising.csv")
View(data)
#1a
model1 <- lm(sales ~ TV, data=data)
summary(model1)
#residuals are getting noticably further away from the abline as fitted values increases, indication of heteroscedasticityâ€”a scenario where the variance of the residuals is not constant across levels of the fitted values.
plot(model1$fitted.values, resid(model1),
xlab = "fitted values", ylab = "residuals",
main = "1a. residuals vs fitted plot")
abline(h = 0, col = "red")
#residual scatter is better, but theres a slight curve and un-uniformity
model1_log <- lm(log(sales) ~ TV, data=data)
summary(model1_log)
#1b
plot(model1_log$fitted.values, resid(model1_log),
xlab = "fitted values", ylab = "residuals",
main = "1b. residuals vs fitted plot for log transform")
abline(h = 0, col = "red")
model1_poly <- lm(log(sales) ~ TV + I(TV^2), data = data)
summary(model1_poly)
plot(model1_log$fitted.values, resid(model1_poly),
xlab = "fitted values", ylab = "residuals",
main = "1b. residuals vs fitted plot for log & poly transforms")
abline(h = 0, col = "red")
#1c - I would be more worried about the divergence at tail ends of our QQ plot if the sample size was smaller and/or if the deviations were more pronounced.
#that being said, minor deviations at the tails of the QQ plot indicate that the residuals might have slight skewness or heavier tails than the normal distribution. This can happen due to the presence of outliers or the influence of high-leverage points that were not entirely addressed by previous transforms.
Impact on Inference
shiny::runApp('GitHub/Divident_Dashboard/Dividend_Dashboard')
runApp('GitHub/Divident_Dashboard/Dividend_Dashboard')
runApp('GitHub/Divident_Dashboard/Dividend_Dashboard')
runApp('GitHub/Divident_Dashboard/Dividend_Dashboard')
runApp('GitHub/Divident_Dashboard/Dividend_Dashboard')
#scatter plot b/w Popularity and Danceability
ggplot(spotify, aes(x = Danceability, y = Popularity)) +
geom_point() +
labs(x = "Danceability", y = "Popularity", title = "Scatter Plot of Danceability vs. Popularity")
library(caret)
#correlation matrix
correlation_matrix <- cor(spotify[c("Popularity", "Danceability", "Energy", "Loudness..dB.", "Liveness", "Valence", "Acousticness", "Speechiness")])
library(ggplot2)
spotify <- read.csv("Spotify-2000.csv")
head(spotify)
#check for missing values
print(paste("Missing values in the dataset:", sum(is.na(spotify))))
colnames(spotify)
library(caret)
#correlation matrix
correlation_matrix <- cor(spotify[c("Popularity", "Danceability", "Energy", "Loudness..dB.", "Liveness", "Valence", "Acousticness", "Speechiness")])
print(correlation_matrix)
#list of highly correlated variables
highly_correlated <- findCorrelation(correlation_matrix, cutoff = 0.5)
highly_correlated
print("Highly correlated variables:")
for (index in highly_correlated) {
print(paste("Index:", index, " - Variable:", colnames(correlation_matrix)[index]))
}
#!!!Steps to show(by slides)
#1. Corellation matrix for both Popularity and Danceability
#2. Original models(with "1" at the end) outputs
#3. VIF check + performance of "2" models (with insignif vars removed)
#4. Adding Genre step, anova results for both
#5. Final models + performance/fit of the model (RSE, R^2)
#!!!
#initial setup
library(tidyverse)
library(GGally)
library(car)
#setwd("/Users/home/Downloads/5. Github Public Repos/Spotify Analysis (Stats 2 Project)")
spotify <- read_csv("data/Spotify-2000.csv")
#select the needed data & rename some columns
spotify_smlr <- spotify %>%
select(Popularity, Acousticness, Speechiness, `Length (Duration)`,
Valence, Liveness, `Loudness (dB)`, Energy,
`Beats Per Minute (BPM)`, `Top Genre`, Danceability) %>%
rename(Loudness = `Loudness (dB)`,
BPM = `Beats Per Minute (BPM)`,
Length = `Length (Duration)`,
Genre = `Top Genre`)
#checking for collinearity of quantitative variables (drop valence and energy)
ggcorr(data = spotify_smlr %>% select(-Genre, -Popularity),
label = TRUE, label_round = 2, label_size = 6,
size = 6, hjust = 0.75,
layout.exp = 1)
#lm without genre
popular_lm1 <- lm(Popularity ~ Acousticness + Speechiness + Length + Valence + Liveness + Loudness + BPM + Danceability + Energy,
data = spotify_smlr)
summary(popular_lm1)
#checking for multi-collinearity(not present):
vif(popular_lm1) #vif should be <5 for all vars
#Popularity continued
#dropping insignifican ones(energy)
popular_lm2 <- lm(Popularity ~ Speechiness + Liveness + Loudness + Danceability,
data = spotify_smlr)
summary(popular_lm2)
#Popularity continued
#adding Genre categorical variable
popular_lm3 <- lm(Popularity ~ Speechiness + Liveness + Loudness + Danceability + Genre,
data = spotify_smlr)
#check if Genre is significant (incremental F-test)
popular_full_lm <- lm(Popularity ~ Speechiness + Liveness + Loudness + Danceability + Genre,
data = spotify_smlr)
popular_null_lm <- lm(Popularity ~ Speechiness + Liveness + Loudness + Danceability,
data = spotify_smlr)
anova(popular_null_lm, popular_full_lm)
#final model
summary(popular_lm3)
#checking for collinearity of quantitative variables (drop energy + acousticness)
ggcorr(data = spotify_smlr %>% select(-Genre, -Danceability),
label = TRUE, label_round = 2, label_size = 6,
size = 6, hjust = 0.75,
layout.exp = 1)
#lm without genre
dance_lm1 <- lm(Danceability ~ Popularity + Speechiness + Length + Valence + Liveness + Loudness + BPM + Energy,
data = spotify_smlr)
summary(dance_lm1)
#checking for multiple-collinearity
vif(dance_lm1)
#dropped length and loudness and energy
dance_lm2 <- lm(Danceability ~ Popularity + Speechiness + Valence + Liveness + BPM,
data = spotify_smlr)
summary(dance_lm2)
#Danceability continued
#adding Genre categorical variable
dance_lm3 <- lm(Danceability ~ Popularity + Speechiness + Valence + Liveness + BPM + Genre,
data = spotify_smlr)
#check if Genre is significant (incremental F-test)
dance_full_lm <- lm(Danceability ~ Popularity + Speechiness + Valence + Liveness + BPM + Genre,
data = spotify_smlr)
dance_null_lm <- lm(Danceability ~ Popularity + Speechiness + Valence + Liveness + BPM,
data = spotify_smlr)
anova(dance_null_lm, dance_full_lm)
#final Danceability model
summary(dance_lm3)
#initial setup
library(tidyverse)
library(GGally)
library(car)
#setwd("/Users/home/Downloads/5. Github Public Repos/Spotify Analysis (Stats 2 Project)")
spotify <- read_csv("data/Spotify-2000.csv")
#select the needed data & rename some columns
spotify_smlr <- spotify %>%
select(Popularity, Acousticness, Speechiness, `Length (Duration)`,
Valence, Liveness, `Loudness (dB)`, Energy,
`Beats Per Minute (BPM)`, `Top Genre`, Danceability) %>%
rename(Loudness = `Loudness (dB)`,
BPM = `Beats Per Minute (BPM)`,
Length = `Length (Duration)`,
Genre = `Top Genre`)
#checking for collinearity of quantitative variables (drop valence and energy)
ggcorr(data = spotify_smlr %>% select(-Genre, -Popularity),
label = TRUE, label_round = 2, label_size = 6,
size = 6, hjust = 0.75,
layout.exp = 1)
#lm without genre
popular_lm1 <- lm(Popularity ~ Acousticness + Speechiness + Length + Valence + Liveness + Loudness + BPM + Danceability + Energy,
data = spotify_smlr)
summary(popular_lm1)
#Popularity continued
#dropping insignifican ones(energy)
popular_lm2 <- lm(Popularity ~ Speechiness + Liveness + Loudness + Danceability,
data = spotify_smlr)
summary(popular_lm2)
#Popularity continued
#adding Genre categorical variable
popular_lm3 <- lm(Popularity ~ Speechiness + Liveness + Loudness + Danceability + Genre,
data = spotify_smlr)
#check if Genre is significant (incremental F-test)
popular_full_lm <- lm(Popularity ~ Speechiness + Liveness + Loudness + Danceability + Genre,
data = spotify_smlr)
popular_null_lm <- lm(Popularity ~ Speechiness + Liveness + Loudness + Danceability,
data = spotify_smlr)
anova(popular_null_lm, popular_full_lm)
packages <- c("shiny", "quantmod", "DT", "highcharter")
install.packages(packages)
shiny::runApp('Spring_2024/Data_Visualization/FinalProject/test')
runApp('Spring_2024/Data_Visualization/FinalProject/test')
runApp('Spring_2024/Data_Visualization/FinalProject/test')
runApp('Spring_2024/Data_Visualization/FinalProject/test')
runApp('Spring_2024/Data_Visualization/FinalProject/test')
runApp('Spring_2024/Data_Visualization/FinalProject/test')
runApp('Spring_2024/Data_Visualization/FinalProject/test')
runApp('Spring_2024/Data_Visualization/FinalProject/test')
runApp('Spring_2024/Data_Visualization/FinalProject/test')
runApp('Spring_2024/Data_Visualization/FinalProject/test')
runApp('Spring_2024/Data_Visualization/FinalProject/test')
runApp('Spring_2024/Data_Visualization/FinalProject/test')
runApp('Spring_2024/Data_Visualization/FinalProject/test')
runApp('Spring_2024/Data_Visualization/FinalProject/test')
runApp('Spring_2024/Data_Visualization/FinalProject/test')
runApp('Spring_2024/Data_Visualization/FinalProject/test')
runApp('Spring_2024/Data_Visualization/FinalProject/test')
runApp('Spring_2024/Data_Visualization/FinalProject/test')
runApp('Spring_2024/Data_Visualization/FinalProject/test')
runApp('Spring_2024/Data_Visualization/FinalProject/test')
runApp('Spring_2024/Data_Visualization/FinalProject/test')
runApp('Spring_2024/Data_Visualization/FinalProject/test')
runApp('Spring_2024/Data_Visualization/FinalProject/test')
runApp('Spring_2024/Data_Visualization/FinalProject/test')
runApp('Spring_2024/Data_Visualization/FinalProject/test')
runApp('Spring_2024/Data_Visualization/FinalProject/test')
runApp('Spring_2024/Data_Visualization/FinalProject/test')
runApp('Spring_2024/Data_Visualization/FinalProject/test')
runApp('Spring_2024/Data_Visualization/FinalProject/test')
runApp('Spring_2024/Data_Visualization/FinalProject/test')
runApp('Spring_2024/Data_Visualization/FinalProject/test')
runApp('Spring_2024/Data_Visualization/FinalProject/test')
runApp('Spring_2024/Data_Visualization/FinalProject/test')
runApp('Spring_2024/Data_Visualization/FinalProject/test')
runApp('Spring_2024/Data_Visualization/FinalProject/test')
runApp('Spring_2024/Data_Visualization/FinalProject/test')
runApp('Spring_2024/Data_Visualization/FinalProject/test')
runApp('Spring_2024/Data_Visualization/FinalProject/test')
runApp('Spring_2024/Data_Visualization/FinalProject/test')
runApp('Spring_2024/Data_Visualization/FinalProject/test')
